{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UImTzmGTIeX9"
      },
      "source": [
        "# Introduction to the OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) aims to provide an easy-to-setup general-intelligence benchmark with various environments. The goal is to standardize how environments are defined in AI research publications to make published research more easily reproducible. The project claims to provide the user with a simple interface. As of June 2017, developers can only use Gym with Python. \n",
        "\n",
        "OpenAI gym is pip-installed onto your local machine. There are a few significant limitations to be aware of:\n",
        "\n",
        "* OpenAI Gym Atari only **directly** supports Linux and Macintosh\n",
        "* OpenAI Gym Atari can be used with Windows; however, it requires a particular [installation procedure](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)\n",
        "* OpenAI Gym can not directly render animated games in Google CoLab.\n",
        "\n",
        "Because OpenAI Gym requires a graphics display, an embedded video is the only way to display Gym in Google CoLab. The presentation of OpenAI Gym game animations in Google CoLab is discussed later in this module.\n",
        "\n",
        "## OpenAI Gym Leaderboard\n",
        "\n",
        "The OpenAI Gym does have a leaderboard, similar to Kaggle; however, the OpenAI Gym's leaderboard is much more informal compared to Kaggle. The user's local machine performs all scoring. As a result, the OpenAI gym's leaderboard is strictly an \"honor system.\"  The leaderboard is maintained in the following GitHub repository:\n",
        "\n",
        "* [OpenAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)\n",
        "\n",
        "You must provide a write-up with sufficient instructions to reproduce your result if you submit a score. A video of your results is suggested but not required.\n",
        "\n",
        "## Looking at Gym Environments\n",
        "\n",
        "The centerpiece of Gym is the environment, which defines the \"game\" in which your reinforcement algorithm will compete. An environment does not need to be a game; however, it describes the following game-like features:\n",
        "* **action space**: What actions can we take on the environment at each step/episode to alter the environment.\n",
        "* **observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
        "\n",
        "Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.\n",
        "\n",
        "* **Agent** - The machine learning program or model that controls the actions.\n",
        "Step - One round of issuing actions that affect the observation space.\n",
        "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective or the episode reaches the maximum number of allowed steps.\n",
        "* **Render** - Gym can render one frame for display after each episode.\n",
        "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
        "* **Non-deterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
        "\n",
        "It is important to note that many gym environments specify that they are not non-deterministic even though they use random numbers to process actions. Based on the gym GitHub issue tracker, a non-deterministic property means a deterministic environment behaves randomly. Even when you give the environment a consistent seed value, this behavior is confirmed. The program can use the seed method of an environment to seed the random number generator for the environment.\n",
        "\n",
        "The Gym library allows us to query some of these attributes from environments. I created the following function to query gym environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cciTuR2MIeX-"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "\n",
        "def query_environment(name):\n",
        "    env = gym.make(name)\n",
        "    spec = gym.spec(name)\n",
        "    print(f\"Action Space: {env.action_space}\")\n",
        "    print(f\"Observation Space: {env.observation_space}\")\n",
        "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "    print(f\"Reward Range: {env.reward_range}\")\n",
        "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRXfAdwFDwUm"
      },
      "source": [
        "We will look at the **MountainCar-v0** environment, which challenges an underpowered car to escape the valley between two mountains.  The following code describes the Mountian Car environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYwy9cjlJjEH",
        "outputId": "10436811-5597-4e04-9ff2-1651256253ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(3)\n",
            "Observation Space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: -110.0\n"
          ]
        }
      ],
      "source": [
        "query_environment(\"MountainCar-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TsQiaGJE3UA"
      },
      "source": [
        "This environment allows three distinct actions: accelerate forward, decelerate, or backward. The observation space contains two continuous (floating point) values, as evident by the box object. The observation space is simply the position and velocity of the car. The car has 200 steps to escape for each episode. You would have to look at the code, but the mountain car receives no incremental reward. The only reward for the vehicle occurs when it escapes the valley.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF4n5cYEMyru",
        "outputId": "188ffd89-54ff-454f-edcf-7228ed4e3176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Max Episode Steps: 500\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 475.0\n"
          ]
        }
      ],
      "source": [
        "query_environment(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwvVKrNebUHJ"
      },
      "source": [
        "The **CartPole-v1** environment challenges the agent to balance a pole while the agent. The environment has an observation space of 4 continuous numbers:\n",
        "\n",
        "* Cart Position\n",
        "* Cart Velocity\n",
        "* Pole Angle\n",
        "* Pole Velocity At Tip\n",
        "\n",
        "To achieve this goal, the agent can take the following actions:\n",
        "\n",
        "* Push cart to the left\n",
        "* Push cart to the right\n",
        "\n",
        "There is also a continuous variant of the mountain car. This version does not simply have the motor on or off. The action space is a single floating-point number for the continuous cart that specifies how much forward or backward force the cart currently utilizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAlaMcJmNSY0",
        "outputId": "9ee20a1b-a3e3-4304-e962-22271f74eb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Box(-1.0, 1.0, (1,), float32)\n",
            "Observation Space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
            "Max Episode Steps: 999\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 90.0\n"
          ]
        }
      ],
      "source": [
        "query_environment(\"MountainCarContinuous-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBrlG1t6ceIa"
      },
      "source": [
        "Note: If you see a warning above, you can safely ignore it; it is a relatively minor bug in OpenAI Gym.\n",
        "\n",
        "Atari games, like breakout, can use an observation space that is either equal to the size of the Atari screen (210x160) or even use the RAM of the Atari (128 bytes) to determine the state of the game.  Yes, that's bytes, not kilobytes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6uBDj0Zyw88",
        "outputId": "0a4665b9-0f50-4970-8827-1b7dbba5fe0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-08 16:34:54--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19583716 (19M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar.1’\n",
            "\n",
            "Roms.rar.1           32%[=====>              ]   6.13M   471KB/s    eta 29s    "
          ]
        }
      ],
      "source": [
        "# HIDE OUTPUT\n",
        "!wget http://www.atarimania.com/roms/Roms.rar \n",
        "!unrar x -o+ /content/Roms.rar >/dev/nul\n",
        "!python -m atari_py.import_roms /content/ROMS >/dev/nul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndTb-9pgJizW"
      },
      "outputs": [],
      "source": [
        "query_environment(\"Breakout-v0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni1rxzmLKAdH"
      },
      "outputs": [],
      "source": [
        "query_environment(\"Breakout-ram-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E253PBGPRuw"
      },
      "source": [
        "## Render OpenAI Gym Environments from CoLab\n",
        "\n",
        "It is possible to visualize the game your agent is playing, even on CoLab. This section provides information on generating a video in CoLab that shows you an episode of the game your agent is playing. I based this video process on suggestions found [here](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t).\n",
        "\n",
        "Begin by installing **pyvirtualdisplay** and **python-opengl**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF92FCzZMWPn"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS7L8kFMLkjN"
      },
      "source": [
        "Next, we install the needed requirements to display an Atari game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78BfQoQKOq7z"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjTHm2SpLz10"
      },
      "source": [
        "Next, we define the functions used to show the video by adding it to the CoLab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9RpF49oOsZj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NATj-kNADT"
      },
      "source": [
        "Now we are ready to play the game.  We use a simple random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDKGJ9A3O8fT"
      },
      "outputs": [],
      "source": [
        "# HIDE OUTPUT\n",
        "#env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "env = wrap_env(gym.make(\"Atlantis-v0\"))\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "\n",
        "    env.render()\n",
        "\n",
        "    # your agent goes here\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "show_video()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sz47aGwAqHjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "DS_D80_LU2_V1.0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}