{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace05fba",
   "metadata": {},
   "source": [
    "* Step 1: Load the demotext.txt text file into a variable and then close the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd376d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "with open('demotext.txt', 'r') as file:\n",
    "    # Read the content of the file into a variable\n",
    "    text_content = file.read()\n",
    "\n",
    "# The file is automatically closed after exiting the 'with' block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f0d03",
   "metadata": {},
   "source": [
    "* Step 2: Do word-wise tokenization and list out the generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a058eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-wise Tokenization:\n",
      "['What', 'is', 'Lorem', 'Ipsum', '?', 'Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text into words\n",
    "word_tokens = word_tokenize(text_content)\n",
    "\n",
    "# Print the generated tokens\n",
    "print(\"Word-wise Tokenization:\")\n",
    "print(word_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251c24",
   "metadata": {},
   "source": [
    "* Step 3: Transform each token into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf4afd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Tokens (Lowercase):\n",
      "['what', 'is', 'lorem', 'ipsum', '?', 'lorem', 'ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'lorem', 'ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'it', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'it', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'letraset', 'sheets', 'containing', 'lorem', 'ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'aldus', 'pagemaker', 'including', 'versions', 'of', 'lorem', 'ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "# Transform each token to lowercase\n",
    "lowercase_tokens = [token.lower() for token in word_tokens]\n",
    "\n",
    "# Print the transformed tokens\n",
    "print(\"\\nTransformed Tokens (Lowercase):\")\n",
    "print(lowercase_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfea0e",
   "metadata": {},
   "source": [
    "* Step 4: Remove stop words from the generated token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4e0a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after removing Stopwords:\n",
      "['lorem', 'ipsum', '?', 'lorem', 'ipsum', 'simply', 'dummy', 'text', 'printing', 'typesetting', 'industry', '.', 'lorem', 'ipsum', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', '1500s', ',', 'unknown', 'printer', 'took', 'galley', 'type', 'scrambled', 'make', 'type', 'specimen', 'book', '.', 'survived', 'five', 'centuries', ',', 'also', 'leap', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'popularised', '1960s', 'release', 'letraset', 'sheets', 'containing', 'lorem', 'ipsum', 'passages', ',', 'recently', 'desktop', 'publishing', 'software', 'like', 'aldus', 'pagemaker', 'including', 'versions', 'lorem', 'ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in lowercase_tokens if token not in stop_words]\n",
    "\n",
    "# Print the tokens after removing stop words\n",
    "print(\"\\nTokens after removing Stopwords:\")\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68d118",
   "metadata": {},
   "source": [
    "* Step 5: Remove extra symbols like commas, full stops, and question marks using a regular expression tokenizer and store them in another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0ac209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after removing Extra Symbols:\n",
      "['lorem', 'ipsum', 'lorem', 'ipsum', 'simply', 'dummy', 'text', 'printing', 'typesetting', 'industry', 'lorem', 'ipsum', 'industry', 's', 'standard', 'dummy', 'text', 'ever', 'since', '1500s', 'unknown', 'printer', 'took', 'galley', 'type', 'scrambled', 'make', 'type', 'specimen', 'book', 'survived', 'five', 'centuries', 'also', 'leap', 'electronic', 'typesetting', 'remaining', 'essentially', 'unchanged', 'popularised', '1960s', 'release', 'letraset', 'sheets', 'containing', 'lorem', 'ipsum', 'passages', 'recently', 'desktop', 'publishing', 'software', 'like', 'aldus', 'pagemaker', 'including', 'versions', 'lorem', 'ipsum']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create a regular expression tokenizer to remove extra symbols\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "cleaned_tokens = tokenizer.tokenize(' '.join(filtered_tokens))\n",
    "\n",
    "# Print the tokens after removing extra symbols\n",
    "print(\"\\nTokens after removing Extra Symbols:\")\n",
    "print(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d240b5",
   "metadata": {},
   "source": [
    "* Step 6: Do bigram and trigram for generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54976be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams:\n",
      "[('lorem', 'ipsum'), ('ipsum', 'lorem'), ('lorem', 'ipsum'), ('ipsum', 'simply'), ('simply', 'dummy'), ('dummy', 'text'), ('text', 'printing'), ('printing', 'typesetting'), ('typesetting', 'industry'), ('industry', 'lorem'), ('lorem', 'ipsum'), ('ipsum', 'industry'), ('industry', 's'), ('s', 'standard'), ('standard', 'dummy'), ('dummy', 'text'), ('text', 'ever'), ('ever', 'since'), ('since', '1500s'), ('1500s', 'unknown'), ('unknown', 'printer'), ('printer', 'took'), ('took', 'galley'), ('galley', 'type'), ('type', 'scrambled'), ('scrambled', 'make'), ('make', 'type'), ('type', 'specimen'), ('specimen', 'book'), ('book', 'survived'), ('survived', 'five'), ('five', 'centuries'), ('centuries', 'also'), ('also', 'leap'), ('leap', 'electronic'), ('electronic', 'typesetting'), ('typesetting', 'remaining'), ('remaining', 'essentially'), ('essentially', 'unchanged'), ('unchanged', 'popularised'), ('popularised', '1960s'), ('1960s', 'release'), ('release', 'letraset'), ('letraset', 'sheets'), ('sheets', 'containing'), ('containing', 'lorem'), ('lorem', 'ipsum'), ('ipsum', 'passages'), ('passages', 'recently'), ('recently', 'desktop'), ('desktop', 'publishing'), ('publishing', 'software'), ('software', 'like'), ('like', 'aldus'), ('aldus', 'pagemaker'), ('pagemaker', 'including'), ('including', 'versions'), ('versions', 'lorem'), ('lorem', 'ipsum')]\n",
      "\n",
      "Trigrams:\n",
      "[('lorem', 'ipsum', 'lorem'), ('ipsum', 'lorem', 'ipsum'), ('lorem', 'ipsum', 'simply'), ('ipsum', 'simply', 'dummy'), ('simply', 'dummy', 'text'), ('dummy', 'text', 'printing'), ('text', 'printing', 'typesetting'), ('printing', 'typesetting', 'industry'), ('typesetting', 'industry', 'lorem'), ('industry', 'lorem', 'ipsum'), ('lorem', 'ipsum', 'industry'), ('ipsum', 'industry', 's'), ('industry', 's', 'standard'), ('s', 'standard', 'dummy'), ('standard', 'dummy', 'text'), ('dummy', 'text', 'ever'), ('text', 'ever', 'since'), ('ever', 'since', '1500s'), ('since', '1500s', 'unknown'), ('1500s', 'unknown', 'printer'), ('unknown', 'printer', 'took'), ('printer', 'took', 'galley'), ('took', 'galley', 'type'), ('galley', 'type', 'scrambled'), ('type', 'scrambled', 'make'), ('scrambled', 'make', 'type'), ('make', 'type', 'specimen'), ('type', 'specimen', 'book'), ('specimen', 'book', 'survived'), ('book', 'survived', 'five'), ('survived', 'five', 'centuries'), ('five', 'centuries', 'also'), ('centuries', 'also', 'leap'), ('also', 'leap', 'electronic'), ('leap', 'electronic', 'typesetting'), ('electronic', 'typesetting', 'remaining'), ('typesetting', 'remaining', 'essentially'), ('remaining', 'essentially', 'unchanged'), ('essentially', 'unchanged', 'popularised'), ('unchanged', 'popularised', '1960s'), ('popularised', '1960s', 'release'), ('1960s', 'release', 'letraset'), ('release', 'letraset', 'sheets'), ('letraset', 'sheets', 'containing'), ('sheets', 'containing', 'lorem'), ('containing', 'lorem', 'ipsum'), ('lorem', 'ipsum', 'passages'), ('ipsum', 'passages', 'recently'), ('passages', 'recently', 'desktop'), ('recently', 'desktop', 'publishing'), ('desktop', 'publishing', 'software'), ('publishing', 'software', 'like'), ('software', 'like', 'aldus'), ('like', 'aldus', 'pagemaker'), ('aldus', 'pagemaker', 'including'), ('pagemaker', 'including', 'versions'), ('including', 'versions', 'lorem'), ('versions', 'lorem', 'ipsum')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "# Generate bigrams and trigrams\n",
    "bigram_result = list(bigrams(cleaned_tokens))\n",
    "trigram_result = list(trigrams(cleaned_tokens))\n",
    "\n",
    "# Print the bigrams and trigrams\n",
    "print(\"\\nBigrams:\")\n",
    "print(bigram_result)\n",
    "print(\"\\nTrigrams:\")\n",
    "print(trigram_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365132e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
