{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxF6VJFcmhqW"
   },
   "source": [
    "**Q1. Problem Statement: Stemming, Lemmatization and Word sense Disambiguation**\n",
    "\n",
    "Perform the following tasks to get an understanding of stemming,\n",
    "lemmatization and Word Sense Disambiguation (WSD) using the Natural Language\n",
    "Tool Kit (NLTK) \n",
    "1.\tDeclare a list of words and perform stemming on each word using PorterStemmer() and LancasterStemmer()\n",
    "2.\tDeclare a sentence and perform lemmatization on each word of the sentence using  WordNetLemmetizer()\n",
    "3.\tDeclare two different sentences with homonyms and perform WSD to fetch the meanings of the homonyms in the context of their respective sentences\n",
    "\n",
    "**Note:** Homonyms are words that have the same spelling and pronunciation, but different meanings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qJF9_b7bVtH"
   },
   "source": [
    "**Step-1:** Importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAvF493Bpsg7"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVschRtfbcCY"
   },
   "source": [
    "**Step-2:** Creating a PortStemmer() and LandcasterStemmer() objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-yxEBj4pn1Q"
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT-U-H_Mcvh5"
   },
   "source": [
    "**Step-3:** Stemming the words using PortStemmer and LandcasterStemmer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iq3XA_zkp0hu",
    "outputId": "c2c73802-256c-4bc9-96fc-609a008decc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Original Word sare:\n",
      "['friend', 'friendship', 'friends', 'friendships', 'stabil', 'destabilize', 'misunderstanding', 'railroad', 'moonlight', 'football']\n",
      "After Stemming the Words using Porter Stemmer:\n",
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"The Original Word sare:\")\n",
    "print(word_list)\n",
    "print(\"After Stemming the Words using Porter Stemmer:\")\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr-aPK0_c764"
   },
   "source": [
    "**Step-4:** Downloading/Importing necessary modules to Lemmatize sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzBla5tjKU2E",
    "outputId": "312b54dd-a303-4976-be94-d3a6a8e8cfc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3UkZMy_dZ8F"
   },
   "source": [
    "**Step-5:** Creating a WordNetLemmetizer() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIAO3InBdYXv"
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec_xj7LVdk9t"
   },
   "source": [
    "**Step-6:** Declaring a sentence as a string and Lemmatizing the same sentance using WordNetLemmatizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chrsmenXIiNM",
    "outputId": "a8db7fd5-75f2-430c-a36f-6c92fe50b766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Original Sentence is:\n",
      "He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\n",
      "After Lemmatizing words using the WordNetLemmatizer:\n",
      "Words               Lemmatized words    \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "print(\"The Original Sentence is:\")\n",
    "print(sentence)\n",
    "print(\"After Lemmatizing words using the WordNetLemmatizer:\")\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Words\",\"Lemmatized words\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khXy0LTDkLhI"
   },
   "source": [
    "**Step-7:** Importing necessary libraries for Word Sense Disambiguation (WSD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6xVJvNeMJi1"
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TWWFsrhkVjR"
   },
   "source": [
    "**Step-9:** Declaring two different sentances with homonyms and performing WSD to fetch the meanings of the homonyms in context to thier respective sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDUnFdftMNTt",
    "outputId": "2e74bc8d-df4f-44a1-9f17-91f7a8324283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-1:\n",
      "This device is used to jam the signal\n",
      "Meaning of the jam word in Sentence-1 is:\n",
      "Synset('throng.v.01') press tightly together or cram\n",
      "Sentence-2:\n",
      "This device is used to jam the signal\n",
      "Meaning of the jam word in Sentence-2 is:\n",
      "Synset('jam.v.05') get stuck and immobilized\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"This device is used to jam the signal\"\n",
    "print(\"Sentence-1:\")\n",
    "print(sentence1)\n",
    "print(\"Meaning of the jam word in Sentence-1 is:\")\n",
    "a1= lesk(word_tokenize('sentence1'),'jam')\n",
    "\n",
    "print(a1,a1.definition())\n",
    "\n",
    "sentence2 = \"This device is used to jam the signal\"\n",
    "print(\"Sentence-2:\")\n",
    "print(sentence2)\n",
    "print(\"Meaning of the jam word in Sentence-2 is:\")\n",
    "a2 = lesk(word_tokenize('I am stuck in a traffic jam'),'jam')\n",
    "\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI0FBO-Um1DO"
   },
   "source": [
    "**Step-9:** Testing WSD with different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_rKlzQNMa8F",
    "outputId": "5f0db3b5-3d11-4d36-da5d-08240967801c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('season.n.02') one of the natural periods into which the year is divided by the equinoxes and solstices or atmospheric conditions\n"
     ]
    }
   ],
   "source": [
    "# testing with some other data \n",
    "\n",
    "b1= lesk(word_tokenize('Apply the salt and other spices to the chicken to season it'),'season')\n",
    "\n",
    "print(b1,b1.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJnDIc0PjwlL",
    "outputId": "232f1dbc-7aa1-4d7b-9208-a208ecb8c759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('season.n.01') a period of the year marked by special events or activities in some field\n"
     ]
    }
   ],
   "source": [
    "# testing with some data\n",
    "\n",
    "b1= lesk(word_tokenize(\"It'll be too humid inside in the rainy season \"),'season')\n",
    "\n",
    "print(b1,b1.definition())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS_D45_DIY_Solution_v1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
